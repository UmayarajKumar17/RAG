{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing document chunk: Failed to connect; did you specify the correct index name?\n",
      "Error processing document chunk: HTTPSConnectionPool(host='quickstart-jr20p2p.svc.aped-4627-b74a.pinecone.io', port=443): Max retries exceeded with url: /vectors/upsert (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001A3DAE83C90>: Failed to resolve 'quickstart-jr20p2p.svc.aped-4627-b74a.pinecone.io' ([Errno 11001] getaddrinfo failed)\"))\n",
      "All PDF files have been processed and indexed successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pinecone\n",
    "import fitz  # PyMuPDF for reading PDF files\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.schema import Document\n",
    "import torch\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import torch\n",
    "import uuid  # For unique chunk IDs\n",
    "import pinecone\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Pinecone API Key and Index Name\n",
    "api_key = \"pcsk_3qCPAf_CX3V2zCFf5ZFVfya8cC1R93wUZQcSPcYvqDGheR8kiLayxw2ZUogQREf4i81y3A\"  # Your Pinecone API key\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "pc = Pinecone(api_key= api_key)\n",
    "index = pc.Index(\"quickstart\")\n",
    "\n",
    "# Initialize Embeddings with GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': device}  # Use GPU if available\n",
    ")\n",
    "\n",
    "# Recursive Text Splitter\n",
    "# Function to read text from PDF\"\"\"\n",
    "\"\"\"def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text\"\"\"\n",
    "\n",
    "# Path to your 7 Harry Potter books in PDF format\n",
    "\"\"\"def load_pdf(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                    glob=\"*.pdf\",\n",
    "                    loader_cls=PyPDFLoader)\n",
    "    \n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents\"\"\"\n",
    "def load_pdfs_from_directory(directory_path):\n",
    "    loader = DirectoryLoader(directory_path, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "data_folder = \"D:/Animal Classification/data\"\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "documents = load_pdfs_from_directory(data_folder)\n",
    "splitted_documents = text_splitter.split_documents(documents)\n",
    "vectors = []\n",
    "for doc in splitted_documents:\n",
    "    try:\n",
    "        clean_source = os.path.basename(doc.metadata.get(\"source\", \"unknown\"))\n",
    "\n",
    "        # Generate unique ID for each chunk\n",
    "        chunk_id = str(uuid.uuid4())\n",
    "        \n",
    "        # Embed the chunk text\n",
    "        embedding = embedding_model.embed_query(doc.page_content)\n",
    "        \n",
    "        # Add vector with cleaned metadata and the full chunk text\n",
    "        vectors.append((chunk_id, embedding, {\n",
    "            \"source\": clean_source,\n",
    "            \"content\": doc.page_content  # Save the actual paragraph for retrieval\n",
    "        }))\n",
    "        \n",
    "        # Batch upsert to Pinecone\n",
    "        if len(vectors) >= 100:  # Batch size\n",
    "            index.upsert(vectors=vectors)\n",
    "            vectors = []  # Clear the vectors list after upsert\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document chunk: {e}\")\n",
    "\n",
    "# Upsert any remaining vectors\n",
    "if vectors:\n",
    "    index.upsert(vectors=vectors)\n",
    "\n",
    "print(\"All PDF files have been processed and indexed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pinecone\n",
    "import fitz \n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import torch\n",
    "import uuid  # For unique chunk IDs\n",
    "import pinecone # PyMuPDF for reading PDF files\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Pinecone API Key and Index Name\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")  # e.g., \"us-west1-gcp\"\n",
    "INDEX_NAME = \"harry-potter-knowledge\"\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\n",
    "if INDEX_NAME not in pinecone.list_indexes():\n",
    "    pinecone.create_index(INDEX_NAME, dimension=384)  # Dimension must match the embedding model\n",
    "index = pinecone.Index(INDEX_NAME)\n",
    "\n",
    "# Initialize Embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "\n",
    "# Recursive Text Splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# Function to read text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Path to your 7 Harry Potter books in PDF format\n",
    "pdf_files = [\n",
    "    \"path/to/harry_potter_book1.pdf\",\n",
    "    \"path/to/harry_potter_book2.pdf\",\n",
    "    \"path/to/harry_potter_book3.pdf\",\n",
    "    \"path/to/harry_potter_book4.pdf\",\n",
    "    \"path/to/harry_potter_book5.pdf\",\n",
    "    \"path/to/harry_potter_book6.pdf\",\n",
    "    \"path/to/harry_potter_book7.pdf\",\n",
    "]\n",
    "\n",
    "# Process each PDF and upload chunks to Pinecone\n",
    "for pdf_file in pdf_files:\n",
    "    try:\n",
    "        # Extract text from PDF\n",
    "        text = extract_text_from_pdf(pdf_file)\n",
    "        # Generate metadata\n",
    "        metadata = {\"source\": os.path.basename(pdf_file)}\n",
    "        # Split text into smaller chunks\n",
    "        splitted_texts = text_splitter.split_documents([{\"text\": text, \"metadata\": metadata}])\n",
    "        \n",
    "        # Embed and upsert into Pinecone\n",
    "        vectors = []\n",
    "        for doc in splitted_texts:\n",
    "            embedding = embedding_model.embed_query(doc[\"text\"])\n",
    "            vectors.append((doc[\"id\"], embedding, doc[\"metadata\"]))\n",
    "        \n",
    "        # Upsert to Pinecone in batches\n",
    "        index.upsert(vectors=vectors)\n",
    "        print(f\"Successfully indexed {len(vectors)} chunks from {pdf_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_file}: {e}\")\n",
    "\n",
    "print(\"All PDF files have been processed and indexed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
